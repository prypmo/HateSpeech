{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a9b4f54",
   "metadata": {},
   "source": [
    "The codes presented here was based on https://mccormickml.com/2019/07/22/BERT-fine-tuning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c12221b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import time\n",
    "import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nlpaug.augmenter.word as naw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2549e9e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting adapter-transformers\n",
      "  Downloading adapter_transformers-3.1.0-py3-none-any.whl (4.8 MB)\n",
      "     ---------------------------------------- 4.8/4.8 MB 6.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\priscila\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from adapter-transformers) (23.0)\n",
      "Collecting numpy>=1.17\n",
      "  Downloading numpy-1.24.1-cp39-cp39-win_amd64.whl (14.9 MB)\n",
      "     ---------------------------------------- 14.9/14.9 MB 8.0 MB/s eta 0:00:00\n",
      "Collecting regex!=2019.12.17\n",
      "  Downloading regex-2022.10.31-cp39-cp39-win_amd64.whl (267 kB)\n",
      "     -------------------------------------- 267.8/267.8 KB 8.3 MB/s eta 0:00:00\n",
      "Collecting huggingface-hub<1.0,>=0.1.0\n",
      "  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
      "     -------------------------------------- 182.4/182.4 KB 5.6 MB/s eta 0:00:00\n",
      "Collecting filelock\n",
      "  Downloading filelock-3.9.0-py3-none-any.whl (9.7 kB)\n",
      "Collecting tqdm>=4.27\n",
      "  Downloading tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\n",
      "     ---------------------------------------- 78.5/78.5 KB 4.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests in c:\\users\\priscila\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from adapter-transformers) (2.28.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\priscila\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from adapter-transformers) (6.0)\n",
      "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
      "  Downloading tokenizers-0.12.1-cp39-cp39-win_amd64.whl (3.3 MB)\n",
      "     ---------------------------------------- 3.3/3.3 MB 7.2 MB/s eta 0:00:00\n",
      "Collecting typing-extensions>=3.7.4.3\n",
      "  Downloading typing_extensions-4.4.0-py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\priscila\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tqdm>=4.27->adapter-transformers) (0.4.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\priscila\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->adapter-transformers) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\priscila\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->adapter-transformers) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\priscila\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->adapter-transformers) (3.0.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\priscila\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->adapter-transformers) (1.26.14)\n",
      "Installing collected packages: tokenizers, typing-extensions, tqdm, regex, numpy, filelock, huggingface-hub, adapter-transformers\n",
      "Successfully installed adapter-transformers-3.1.0 filelock-3.9.0 huggingface-hub-0.11.1 numpy-1.24.1 regex-2022.10.31 tokenizers-0.12.1 tqdm-4.64.1 typing-extensions-4.4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\Priscila\\AppData\\Local\\Programs\\Python\\Python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install adapter-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57bcb77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelPath = \"Model/BertBaseMultUncased/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb76c4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaning_data = pd.read_csv('Data/Tweet_Processed_DataCleaning_Done.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3cdfc03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>euedsonduarte lilovlog jairbolsonaro exatamen...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a china fecha o primeiro laboratorio do mundo ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>janeiro china mente sobre a de mortos nos caso...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nivel de poluicao na china cai drasticamente a...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>eikebatista os que cruzam os oceanos trazem u...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Tweet  Label\n",
       "0   euedsonduarte lilovlog jairbolsonaro exatamen...    0.0\n",
       "1  a china fecha o primeiro laboratorio do mundo ...    0.0\n",
       "2  janeiro china mente sobre a de mortos nos caso...    0.0\n",
       "3  nivel de poluicao na china cai drasticamente a...    0.0\n",
       "4   eikebatista os que cruzam os oceanos trazem u...    0.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaning_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7d1b3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = cleaning_data['Tweet']\n",
    "y_data = cleaning_data['Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f507c053",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, stratify=y_data, random_state=43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65bf139",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.2, stratify=y_train, random_state=43)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da71c93",
   "metadata": {},
   "source": [
    "### Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49cb75a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install nlpaug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e1ef8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.count_nonzero(y_train == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8087aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.count_nonzero(y_train == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cceeb1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_y = np.where(y_data == 1)\n",
    "indices_y = np.asarray(indices_y)\n",
    "indices_y = indices_y.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6eefd10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2034,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_aug = np.take(x_data, indices_y)\n",
    "x_train_aug.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ce9e79b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10       a teoria da conspiracao de hoje ta sendo a chi...\n",
       "19       sem comentarios - video - pastor diz que china...\n",
       "22        biakicis gleisi vai para china sua coronaviru...\n",
       "35       melhor explicacao - coronavirus e igual macarr...\n",
       "37       c a china enviar coronavirus pra ca, enviaremo...\n",
       "                               ...                        \n",
       "24131     rodrigomaia voce nasceu no chile deveria volt...\n",
       "24139    mano imagina gestar uma crianca por nove meses...\n",
       "24144     jbsantz covid xing ling kkkkk essa prof gosta...\n",
       "24179    covid- o virus chines ou o crime chines? veja ...\n",
       "24188     dmaia jairbolsonaro vou deixar o mesmo recado...\n",
       "Name: Tweet, Length: 2034, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c5d1e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_aug = np.take(x_data, indices_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1bb3d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_syn = naw.SynonymAug(aug_src='wordnet', model_path=modelPath, lang ='por')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa45974b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in x_train_aug:\n",
    "    for ii in range(10):\n",
    "        text = np.array(aug_syn.augment(sentence))\n",
    "        np.append(x_data, text)#lst.append(text)\n",
    "        label = np.array(1)\n",
    "        np.append(y_data, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d85c991",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1b455a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.DataFrame({'text': x_data, 'labels': y_data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "37f3cf94",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.to_csv('DataAugmentation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba87045e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c972f4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.iloc[[2840]].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8511e686",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data = pd.DataFrame({'text': x_valid, 'labels': y_valid})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c70c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8978c6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6791c454",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_train = y_train.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44302865",
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_test = x_test.values\n",
    "#y_test = y_test.values\n",
    "\n",
    "#x_valid = x_valid.values\n",
    "#y_valid = y_valid.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f683093f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff49418",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "#If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89ef5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preProcessing(data, maxLength, tokenizer):    \n",
    "    for tweet in data:\n",
    "        encoded = tokenizer.encode_plus(\n",
    "                    tweet, #Sentence to be tokenized\n",
    "                    add_special_tokens=True, #Adding [CLS] a token added to beggining of the setence\n",
    "                                             #and [SEP] a token added to end of the setence\n",
    "                    max_length=maxLength, #the max size of the setence\n",
    "                    padding='max_length',#pad_to_max_length = True, #Adding [PAD] a token that represents the real sentence(when the setence is  \n",
    "                                              #smaller than the max size the spaces will be completed with this token)                                              \n",
    "                    return_attention_mask=True, #An array of 0 and 1 indicating which tokens are [PAD](space in blank) \n",
    "                                                #and the tokens belonging to the sentence\n",
    "                    return_tensors = 'pt', #Return pytorch tensors(the same as numpy array)\n",
    "        )\n",
    "        \n",
    "        \n",
    "        input_ids.append(encoded['input_ids'])\n",
    "        attention_masks.append(encoded['attention_mask'])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6234c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading locally the bert model\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(modelPath, local_files_only=True, do_lower_case=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d61451f",
   "metadata": {},
   "source": [
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "# Tokenizing the senteces of the train dataset\n",
    "preProcessing(x_train, 512, bert_tokenizer)\n",
    "\n",
    "#torch.cat concatenate the tensors \n",
    "inputIdTrain = torch.cat(input_ids, dim=0)\n",
    "attentionMaskTrain = torch.cat(attention_masks, dim=0)\n",
    "labelsTrain = torch.tensor(y_train)\n",
    "labelsTrain = torch.tensor(y_train).unsqueeze(1)\n",
    "labelsTrain = labelsTrain.to(torch.int64)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297e7c04",
   "metadata": {},
   "source": [
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "# Tokenizing the senteces of the train dataset\n",
    "preProcessing(x_valid, 512, bert_tokenizer)\n",
    "\n",
    "#torch.cat concatenate the tensors \n",
    "inputIdValid = torch.cat(input_ids, dim=0)\n",
    "attentionMaskValid = torch.cat(attention_masks, dim=0)\n",
    "labelsValid = torch.tensor(y_valid)\n",
    "labelsValid = torch.tensor(y_valid).unsqueeze(1)\n",
    "labelsValid = labelsValid.to(torch.int64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c0d36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c24f09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c18ddf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ff595c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TensorDataset encapsulates the data\n",
    "#train_data = TensorDataset(inputIdTrain, attentionMaskTrain, labelsTrain)\n",
    "#valid_data = TensorDataset(inputIdValid, attentionMaskValid, labelsValid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b360070a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_pandas(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bbaf2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataset = Dataset.from_pandas(valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89e6841",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processingToken(batch):\n",
    "  \"\"\"Encodes a batch of input data using the model tokenizer.\"\"\"\n",
    "  return bert_tokenizer(batch[\"text\"], max_length=512, truncation=True, padding=\"max_length\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f4dd46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the input data\n",
    "train_dataset = train_dataset.map(processingToken, batched=True)\n",
    "# Transform to pytorch tensors and only output the required columns\n",
    "train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f2b2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the input data\n",
    "valid_dataset = valid_dataset.map(processingToken, batched=True)\n",
    "# Transform to pytorch tensors and only output the required columns\n",
    "valid_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e9d0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig, BertModelWithHeads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3865b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = BertConfig.from_pretrained(\n",
    "    modelPath, \n",
    "    local_files_only=True,\n",
    "    num_labels=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1a5c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertModelWithHeads.from_pretrained(\n",
    "    modelPath, \n",
    "    local_files_only=True,\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec71c141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new adapter\n",
    "model.add_adapter(\"rotten_tomatoes\")\n",
    "# Add a matching classification head\n",
    "model.add_classification_head(\n",
    "    \"rotten_tomatoes\",\n",
    "    num_labels=2\n",
    "  )\n",
    "# Activate the adapter\n",
    "model.train_adapter(\"rotten_tomatoes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2450f266",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from transformers import TrainingArguments, AdapterTrainer, EvalPrediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b931ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    learning_rate=1e-4,\n",
    "    num_train_epochs=6,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    logging_steps=200,\n",
    "    output_dir=\"./training_output\",\n",
    "    overwrite_output_dir=True,\n",
    "    # The next line is important to ensure the dataset labels are properly passed to the model\n",
    "    remove_unused_columns=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f78e3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(p: EvalPrediction):\n",
    "  preds = np.argmax(p.predictions, axis=1)\n",
    "  return {\"acc\": (preds == p.label_ids).mean()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d62bc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = AdapterTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    "    compute_metrics=compute_accuracy,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3661cf5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63898a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda0247e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b22eac5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146dbce1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cb085e83",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f783c676",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af85375",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342149e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90502326",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c6c8b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
